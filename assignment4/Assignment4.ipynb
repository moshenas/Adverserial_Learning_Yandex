{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Defensive Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mnist_model(num_classes):\n",
    "    \n",
    "    activation = 'relu'\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols, img_colors = 28, 28, 1\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(8, kernel_size=(3, 3), input_shape=(img_rows, img_cols, img_colors), activation=activation))\n",
    "    model.add(layers.Conv2D(8, (3, 3), activation=activation))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes))\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_classes, train_images, train_labels, test_images, test_labels, train_temp):\n",
    "    \n",
    "    batch_size = 128\n",
    "    maxepoches = 12\n",
    "    learning_rate = 0.1\n",
    "    lr_decay = 1e-6\n",
    "    lr_drop = 20\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "    def CE_with_temperture(y_true, y_pred):\n",
    "        loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        return loss(y_true, y_pred)\n",
    "    \n",
    "    model = build_mnist_model(num_classes)\n",
    "\n",
    "    model.compile(loss=CE_with_temperture,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "    history = model.fit(train_images, train_labels,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=maxepoches,\n",
    "                        verbose=1,\n",
    "                        validation_data=(test_images, test_labels),\n",
    "                        callbacks=[reduce_lr])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A simple utility funcion for evaluating the success of an attack\n",
    "'''\n",
    "def TestAttack(model, adv_images, orig_images, true_labels, target_labels=None, targeted=False):\n",
    "    score = model.evaluate(adv_images, true_labels, verbose=0)\n",
    "    print('Test loss: {:.2f}'.format(score[0]))\n",
    "    print('Successfully moved out of source class: {:.2f}'.format( 1 - score[1]))\n",
    "    \n",
    "    if targeted:\n",
    "        score = model.evaluate(adv_images, target, verbose=0)\n",
    "        print('Test loss: {:.2f}'.format(score[0]))\n",
    "        print('Successfully perturbed to target class: {:.2f}'.format(score[1]))\n",
    "    \n",
    "    dist = np.mean(np.sqrt(np.mean(np.square(adv_images - orig_images), axis=(1,2,3))))\n",
    "    print('Mean perturbation distance: {:.2f}'.format(dist))\n",
    "    \n",
    "    index = 10\n",
    "    img = adv_images[index].reshape(28, 28)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Fast Gradient Sign Method implementation - perturb all input features by an epsilon sized step in \n",
    "    the direction of loss gradient\n",
    "'''\n",
    "def FastGradientSignMethod(model, images, labels, epsilon=0.3, verbose=False):\n",
    "\n",
    "    # The GradientTape is the context at which we can explicitly ask for gradient calculation\n",
    "    # We define the relevant tensors inside that context, and ask for the gradient calculation outside of it\n",
    "    with tf.GradientTape() as grad:\n",
    "        true_label_tensor = tf.Variable(labels, dtype=tf.float32)\n",
    "        input_tensor = tf.Variable(images, dtype=tf.float32)\n",
    "        predicted = model(input_tensor)\n",
    "        adv_loss = keras.losses.categorical_crossentropy(true_label_tensor, predicted)\n",
    "    adv_grads = grad.gradient(adv_loss, input_tensor)\n",
    "\n",
    "    # Finally, the FGSM formula is rather straight forward x`= x + epsilon * sign(loss(x,y))\n",
    "    delta = tf.cast(tf.sign(adv_grads), tf.float32)\n",
    "    if verbose:\n",
    "        print('Gradient map')\n",
    "        image = delta.numpy()[10]\n",
    "        plt.imshow(image.reshape((28,28)), cmap='RdYlGn')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    delta = tf.multiply(epsilon, delta)\n",
    "    adv_out = input_tensor + delta\n",
    "    return adv_out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Targeted Gradient Sign Method implementation - A targeted variant of the FGSM attack\n",
    "    here we minimize the loss with respect to the target class, as opposed to maximizing the loss with respect\n",
    "    to the source class\n",
    "'''\n",
    "def TargetedGradientSignMethod(model, images, target, epsilon=0.3):\n",
    "    # The GradientTape is the context at which we can explicitly ask for gradient calculation\n",
    "    # We define the relevant tensors inside that context, and ask for the gradient calculation outside of it\n",
    "    with tf.GradientTape() as grad:\n",
    "        target_label_tensor = tf.Variable(target, dtype=tf.float32)\n",
    "        input_tensor = tf.Variable(images, dtype=tf.float32)\n",
    "        predicted = model(input_tensor)\n",
    "        adv_loss = keras.losses.categorical_crossentropy(target_label_tensor, predicted)\n",
    "    adv_grads = grad.gradient(adv_loss, input_tensor)\n",
    "\n",
    "    # Finally, the FGSM formula is rather straight forward x`= x + epsilon * sign(loss(x,y))\n",
    "    delta = tf.multiply(epsilon, tf.cast(tf.sign(adv_grads), dtype=tf.float32))\n",
    "    adv_out = input_tensor - delta\n",
    "    return adv_out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_L2(model, images, labels, epsilon=0.1, iter_eps = 0.05, iterations=10, min_x=0.0, max_x=1.0, targeted=False):\n",
    "    \n",
    "    adv_out = images\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print('Iteration:', iteration)\n",
    "        # Perturb the input\n",
    "        if targeted:\n",
    "            adv_out = TargetedGradientSignMethod(model, adv_out, labels, epsilon=iter_eps)\n",
    "        else:\n",
    "            adv_out = FastGradientSignMethod(model, adv_out, labels, epsilon=iter_eps)\n",
    "            \n",
    "        # Project the perturbation to the epsilon ball (L2 projection)\n",
    "        perturbation = adv_out - images\n",
    "        norm = np.sum(np.square(perturbation), axis=(1,2,3), keepdims=True)\n",
    "        norm = np.sqrt(np.maximum(10e-12, norm))\n",
    "        factor = np.minimum(1, np.divide(epsilon, norm))\n",
    "        adv_out = np.clip(images + perturbation * factor, min_x, max_x)\n",
    "    \n",
    "    return adv_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defensive distillation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defensive_distilation(num_classes, train_data, train_labels, test_data, test_labels, temp):\n",
    "    \"\"\"\n",
    "    This function implements defensive distilation method\n",
    "    \n",
    "    Returns:\n",
    "        the trained teacher and student models\n",
    "    \"\"\"\n",
    "    \n",
    "    teacher_model = train_model(num_classes, train_data, train_labels, test_data, test_labels, temp)\n",
    "    train_teacher_probabilities  = keras.activations.softmax(teacher_model(train_data) / temp, axis=-1)\n",
    "    \n",
    "    student_model = train_model(num_classes, train_data, train_teacher_probabilities, test_data, test_labels, temp)\n",
    "    \n",
    "    return teacher_model, student_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_train,x_test):\n",
    "    x_train -= x_train.min()\n",
    "    x_train /= x_train.max()\n",
    "    x_test -= x_test.min()\n",
    "    x_test /= x_test.max()\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "img_rows, img_cols, img_colors = 28, 28, 1\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "train_images, test_images = normalize(train_images, test_images)\n",
    "    \n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and attacking "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
